{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Datase\n"
      ],
      "metadata": {
        "id": "6cCsbdPOKnnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # Pandas untuk manipulasi dan analisis data\n",
        "file_name = 'ulasan_aplikasi.csv'\n",
        "\n",
        "df_reviews = pd.read_csv(file_name) # Membaca file CSV yang telah disimpan dan memverifikasi isinya\n",
        "print(df_reviews.head()) # Menampilkan beberapa baris pertama dari DataFrame\n",
        "print(df_reviews.info()) # Menampilkan informasi tentang DataFrame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jwZxCvvJzQd",
        "outputId": "c104c88e-2e88-4a05-e82d-49f71a4d3d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Review\n",
            "0  Plis kemaren2 bisa edit video yg kualitas vide...\n",
            "1  Apalah lagi edit video, video nya tidak bisa d...\n",
            "2  apk sejuta umat,awalnya bagus banget karena fi...\n",
            "3  Dulu saya sangat suka sekali..tapi sekarang ko...\n",
            "4  apk nya bagus, oke, tapi akhir akhir ini banya...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100000 entries, 0 to 99999\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   Review  100000 non-null  object\n",
            "dtypes: object(1)\n",
            "memory usage: 781.4+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing"
      ],
      "metadata": {
        "id": "9q2H-sC0M9Ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re # Modul untuk bekerja dengan ekspresi reguler\n",
        "import string # Berisi konstanta string, seperti tanda baca\n",
        "from nltk.corpus import stopwords  # Daftar kata-kata berhenti dalam teks\n",
        "from nltk.tokenize import word_tokenize # Tokenisasi teks"
      ],
      "metadata": {
        "id": "gS0O2pX3d4xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory  # Stemming (penghilangan imbuhan kata) dalam bahasa Indonesia\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory  # Menghapus kata-kata berhenti dalam bahasa Indonesia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f610be0-fa71-4e1b-e0a3-22e1c68b21e8",
        "id": "vt-U_n4NdxAk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m163.8/209.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sastrawi\n",
            "Successfully installed sastrawi-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengunduh resource dari NLTK yang diperlukan untuk prapemprosesan data\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e42b4879-30fe-4507-fc93-00def1d7e3f0",
        "id": "WQLB46C2fAjU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYHrCO6EAm2X"
      },
      "outputs": [],
      "source": [
        "# Fungsi untuk membersihkan teks (case folding, menghapus mention, hashtag, RT, link, angka, dan tanda baca)\n",
        "def clean_and_casefold(text):\n",
        "    text = re.sub(r'@[A-Za-z0-9]+|#[A-Za-z0-9]+|RT[\\s]|http\\S+|\\d+|[^\\w\\s]', '', text)\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = text.strip()\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Fungsi untuk tokenisasi dan penghapusan stopwords\n",
        "def tokenize_and_filter(text):\n",
        "    stopwords_indonesia = set(stopwords.words('indonesian'))\n",
        "    stopwords_english = set(stopwords.words('english'))\n",
        "    custom_stopwords = {'iya', 'yaa', 'gak', 'nya', 'na', 'sih', 'ku', 'di', 'ga', 'ya', 'gaa', 'loh', 'kah', 'woi', 'woii', 'woy'}\n",
        "    stopwords_all = stopwords_indonesia.union(stopwords_english).union(custom_stopwords)\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word not in stopwords_all]\n",
        "    return filtered_words\n",
        "\n",
        "# Fungsi untuk melakukan stemming pada teks\n",
        "def stem_text(text):\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in text]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "# Fungsi untuk mengubah daftar kata menjadi kalimat\n",
        "def to_sentence(list_words):\n",
        "    return ' '.join(list_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA5BnScynn2h"
      },
      "outputs": [],
      "source": [
        "slangwords = {\"@\": \"di\", \"abis\": \"habis\", \"wtb\": \"beli\", \"masi\": \"masih\", \"wts\": \"jual\", \"wtt\": \"tukar\", \"bgt\": \"banget\", \"maks\": \"maksimal\", \"plisss\": \"tolong\", \"bgttt\": \"banget\", \"indo\": \"indonesia\", \"bgtt\": \"banget\", \"ad\": \"ada\", \"rv\": \"redvelvet\", \"plis\": \"tolong\", \"pls\": \"tolong\", \"cr\": \"sumber\", \"cod\": \"bayar ditempat\", \"adlh\": \"adalah\", \"afaik\": \"as far as i know\", \"ahaha\": \"haha\", \"aj\": \"saja\", \"ajep-ajep\": \"dunia gemerlap\", \"ak\": \"saya\", \"akika\": \"aku\", \"akkoh\": \"aku\", \"akuwh\": \"aku\", \"alay\": \"norak\", \"alow\": \"halo\", \"ambilin\": \"ambilkan\", \"ancur\": \"hancur\", \"anjrit\": \"anjing\", \"anter\": \"antar\", \"ap2\": \"apa-apa\", \"apasih\": \"apa sih\", \"apes\": \"sial\", \"aps\": \"apa\", \"aq\": \"saya\", \"aquwh\": \"aku\", \"asbun\": \"asal bunyi\", \"aseekk\": \"asyik\", \"asekk\": \"asyik\", \"asem\": \"asam\", \"aspal\": \"asli tetapi palsu\", \"astul\": \"asal tulis\", \"ato\": \"atau\", \"au ah\": \"tidak mau tahu\", \"awak\": \"saya\", \"ay\": \"sayang\", \"ayank\": \"sayang\", \"b4\": \"sebelum\", \"bakalan\": \"akan\", \"bandes\": \"bantuan desa\", \"bangedh\": \"banget\", \"banpol\": \"bantuan polisi\", \"banpur\": \"bantuan tempur\", \"basbang\": \"basi\", \"bcanda\": \"bercanda\", \"bdg\": \"bandung\", \"begajulan\": \"nakal\", \"beliin\": \"belikan\", \"bencong\": \"banci\", \"bentar\": \"sebentar\", \"ber3\": \"bertiga\", \"beresin\": \"membereskan\", \"bete\": \"bosan\", \"beud\": \"banget\", \"bg\": \"abang\", \"bgmn\": \"bagaimana\", \"bgt\": \"banget\", \"bijimane\": \"bagaimana\", \"bintal\": \"bimbingan mental\", \"bkl\": \"akan\", \"bknnya\": \"bukannya\", \"blegug\": \"bodoh\", \"blh\": \"boleh\", \"bln\": \"bulan\", \"blum\": \"belum\", \"bnci\": \"benci\", \"bnran\": \"yang benar\", \"bodor\": \"lucu\", \"bokap\": \"ayah\", \"boker\": \"buang air besar\", \"bokis\": \"bohong\", \"boljug\": \"boleh juga\", \"bonek\": \"bocah nekat\", \"boyeh\": \"boleh\", \"br\": \"baru\", \"brg\": \"bareng\", \"bro\": \"saudara laki-laki\", \"bru\": \"baru\", \"bs\": \"bisa\", \"bsen\": \"bosan\", \"bt\": \"buat\", \"btw\": \"ngomong-ngomong\", \"buaya\": \"tidak setia\", \"bubbu\": \"tidur\", \"bubu\": \"tidur\", \"bumil\": \"ibu hamil\", \"bw\": \"bawa\", \"bwt\": \"buat\", \"byk\": \"banyak\", \"byrin\": \"bayarkan\", \"cabal\": \"sabar\", \"cadas\": \"keren\", \"calo\": \"makelar\", \"can\": \"belum\", \"capcus\": \"pergi\", \"caper\": \"cari perhatian\", \"ce\": \"cewek\", \"cekal\": \"cegah tangkal\", \"cemen\": \"penakut\", \"cengengesan\": \"tertawa\", \"cepet\": \"cepat\", \"cew\": \"cewek\", \"chuyunk\": \"sayang\", \"cimeng\": \"ganja\", \"cipika cipiki\": \"cium pipi kanan cium pipi kiri\", \"ciyh\": \"sih\", \"ckepp\": \"cakep\", \"ckp\": \"cakep\", \"cmiiw\": \"correct me if i'm wrong\", \"cmpur\": \"campur\", \"cong\": \"banci\", \"conlok\": \"cinta lokasi\", \"cowwyy\": \"maaf\", \"cp\": \"siapa\", \"cpe\": \"capek\", \"cppe\": \"capek\", \"cucok\": \"cocok\", \"cuex\": \"cuek\", \"cumi\": \"Cuma miscall\", \"cups\": \"culun\", \"curanmor\": \"pencurian kendaraan bermotor\", \"curcol\": \"curahan hati colongan\", \"cwek\": \"cewek\", \"cyin\": \"cinta\", \"d\": \"di\", \"dah\": \"deh\", \"dapet\": \"dapat\", \"de\": \"adik\", \"dek\": \"adik\", \"demen\": \"suka\", \"deyh\": \"deh\", \"dgn\": \"dengan\", \"diancurin\": \"dihancurkan\", \"dimaafin\": \"dimaafkan\", \"dimintak\": \"diminta\", \"disono\": \"di sana\", \"dket\": \"dekat\", \"dkk\": \"dan kawan-kawan\", \"dll\": \"dan lain-lain\", \"dlu\": \"dulu\", \"dngn\": \"dengan\", \"dodol\": \"bodoh\", \"doku\": \"uang\", \"dongs\": \"dong\", \"dpt\": \"dapat\", \"dri\": \"dari\", \"drmn\": \"darimana\", \"drtd\": \"dari tadi\", \"dst\": \"dan seterusnya\", \"dtg\": \"datang\", \"duh\": \"aduh\", \"duren\": \"durian\", \"ed\": \"edisi\", \"egp\": \"emang gue pikirin\", \"eke\": \"aku\", \"elu\": \"kamu\", \"emangnya\": \"memangnya\", \"emng\": \"memang\", \"endak\": \"tidak\", \"enggak\": \"tidak\", \"envy\": \"iri\", \"ex\": \"mantan\", \"fax\": \"facsimile\", \"fifo\": \"first in first out\", \"folbek\": \"follow back\", \"fyi\": \"sebagai informasi\", \"gaada\": \"tidak ada uang\", \"gag\": \"tidak\", \"gaje\": \"tidak jelas\", \"gak papa\": \"tidak apa-apa\", \"gan\": \"juragan\", \"gaptek\": \"gagap teknologi\", \"gatek\": \"gagap teknologi\", \"gawe\": \"kerja\", \"gbs\": \"tidak bisa\", \"gebetan\": \"orang yang disuka\", \"geje\": \"tidak jelas\", \"gepeng\": \"gelandangan dan pengemis\", \"ghiy\": \"lagi\", \"gile\": \"gila\", \"gimana\": \"bagaimana\", \"gino\": \"gigi nongol\", \"githu\": \"gitu\", \"gj\": \"tidak jelas\", \"gmana\": \"bagaimana\", \"gn\": \"begini\", \"goblok\": \"bodoh\", \"golput\": \"golongan putih\", \"gowes\": \"mengayuh sepeda\", \"gpny\": \"tidak punya\", \"gr\": \"gede rasa\", \"gretongan\": \"gratisan\", \"gtau\": \"tidak tahu\", \"gua\": \"saya\", \"guoblok\": \"goblok\", \"gw\": \"saya\", \"ha\": \"tertawa\", \"haha\": \"tertawa\", \"hallow\": \"halo\", \"hankam\": \"pertahanan dan keamanan\", \"hehe\": \"he\", \"helo\": \"halo\", \"hey\": \"hai\", \"hlm\": \"halaman\", \"hny\": \"hanya\", \"hoax\": \"isu bohong\", \"hr\": \"hari\", \"hrus\": \"harus\", \"hubdar\": \"perhubungan darat\", \"huff\": \"mengeluh\", \"hum\": \"rumah\", \"humz\": \"rumah\", \"ilang\": \"hilang\", \"ilfil\": \"tidak suka\", \"imho\": \"in my humble opinion\", \"imoetz\": \"imut\", \"item\": \"hitam\", \"itungan\": \"hitungan\", \"iye\": \"iya\", \"ja\": \"saja\", \"jadiin\": \"jadi\", \"jaim\": \"jaga image\", \"jayus\": \"tidak lucu\", \"jdi\": \"jadi\", \"jem\": \"jam\", \"jga\": \"juga\", \"jgnkan\": \"jangankan\", \"jir\": \"anjing\", \"jln\": \"jalan\", \"jomblo\": \"tidak punya pacar\", \"jubir\": \"juru bicara\", \"jutek\": \"galak\", \"k\": \"ke\", \"kab\": \"kabupaten\", \"kabor\": \"kabur\", \"kacrut\": \"kacau\", \"kadiv\": \"kepala divisi\", \"kagak\": \"tidak\", \"kalo\": \"kalau\", \"kampret\": \"sialan\", \"kamtibmas\": \"keamanan dan ketertiban masyarakat\", \"kamuwh\": \"kamu\", \"kanwil\": \"kantor wilayah\", \"karna\": \"karena\", \"kasubbag\": \"kepala subbagian\", \"katrok\": \"kampungan\", \"kayanya\": \"kayaknya\", \"kbr\": \"kabar\", \"kdu\": \"harus\", \"kec\": \"kecamatan\", \"kejurnas\": \"kejuaraan nasional\", \"kekeuh\": \"keras kepala\", \"kel\": \"kelurahan\", \"kemaren\": \"kemarin\", \"kepengen\": \"mau\", \"kepingin\": \"mau\", \"kepsek\": \"kepala sekolah\", \"kesbang\": \"kesatuan bangsa\", \"kesra\": \"kesejahteraan rakyat\", \"ketrima\": \"diterima\", \"kgiatan\": \"kegiatan\", \"kibul\": \"bohong\", \"kimpoi\": \"kawin\", \"kl\": \"kalau\", \"klianz\": \"kalian\", \"kloter\": \"kelompok terbang\", \"klw\": \"kalau\", \"km\": \"kamu\", \"kmps\": \"kampus\", \"kmrn\": \"kemarin\", \"knal\": \"kenal\", \"knp\": \"kenapa\", \"kodya\": \"kota madya\", \"komdis\": \"komisi disiplin\", \"komsov\": \"komunis sovyet\", \"kongkow\": \"kumpul bareng teman-teman\", \"kopdar\": \"kopi darat\", \"korup\": \"korupsi\", \"kpn\": \"kapan\", \"krenz\": \"keren\", \"krm\": \"kirim\", \"kt\": \"kita\", \"ktmu\": \"ketemu\", \"ktr\": \"kantor\", \"kuper\": \"kurang pergaulan\", \"kw\": \"imitasi\", \"kyk\": \"seperti\", \"la\": \"lah\", \"lam\": \"salam\", \"lamp\": \"lampiran\", \"lanud\": \"landasan udara\", \"latgab\": \"latihan gabungan\", \"lebay\": \"berlebihan\", \"leh\": \"boleh\", \"lelet\": \"lambat\", \"lemot\": \"lambat\", \"lgi\": \"lagi\", \"lgsg\": \"langsung\", \"liat\": \"lihat\", \"litbang\": \"penelitian dan pengembangan\", \"lmyn\": \"lumayan\", \"lo\": \"kamu\", \"loe\": \"kamu\", \"lola\": \"lambat berfikir\", \"louph\": \"cinta\", \"low\": \"kalau\", \"lp\": \"lupa\", \"luber\": \"langsung, umum, bebas, dan rahasia\", \"luchuw\": \"lucu\", \"lum\": \"belum\", \"luthu\": \"lucu\", \"lwn\": \"lawan\", \"maacih\": \"terima kasih\", \"mabal\": \"bolos\", \"macem\": \"macam\", \"macih\": \"masih\", \"maem\": \"makan\", \"magabut\": \"makan gaji buta\", \"maho\": \"homo\", \"mak jang\": \"kaget\", \"maksain\": \"memaksa\", \"malem\": \"malam\", \"mam\": \"makan\", \"maneh\": \"kamu\", \"maniez\": \"manis\", \"mao\": \"mau\", \"masukin\": \"masukkan\", \"melu\": \"ikut\", \"mepet\": \"dekat sekali\", \"mgu\": \"minggu\", \"migas\": \"minyak dan gas bumi\", \"mikol\": \"minuman beralkohol\", \"miras\": \"minuman keras\", \"mlah\": \"malah\", \"mngkn\": \"mungkin\", \"mo\": \"mau\", \"mokad\": \"mati\", \"moso\": \"masa\", \"mpe\": \"sampai\", \"msk\": \"masuk\", \"mslh\": \"masalah\", \"mt\": \"makan teman\", \"mubes\": \"musyawarah besar\", \"mulu\": \"melulu\", \"mumpung\": \"selagi\", \"munas\": \"musyawarah nasional\", \"muntaber\": \"muntah dan berak\", \"musti\": \"mesti\", \"muupz\": \"maaf\", \"mw\": \"now watching\", \"n\": \"dan\", \"nanam\": \"menanam\", \"nanya\": \"bertanya\", \"napa\": \"kenapa\", \"napi\": \"narapidana\", \"napza\": \"narkotika, alkohol, psikotropika, dan zat adiktif \", \"narkoba\": \"narkotika, psikotropika, dan obat terlarang\", \"nasgor\": \"nasi goreng\", \"nda\": \"tidak\", \"ndiri\": \"sendiri\", \"ne\": \"ini\", \"nekolin\": \"neokolonialisme\", \"nembak\": \"menyatakan cinta\", \"ngabuburit\": \"menunggu berbuka puasa\", \"ngaku\": \"mengaku\", \"ngambil\": \"mengambil\", \"nganggur\": \"tidak punya pekerjaan\", \"ngapah\": \"kenapa\", \"ngaret\": \"terlambat\", \"ngasih\": \"memberikan\", \"ngebandel\": \"berbuat bandel\", \"ngegosip\": \"bergosip\", \"ngeklaim\": \"mengklaim\", \"ngeksis\": \"menjadi eksis\", \"ngeles\": \"berkilah\", \"ngelidur\": \"menggigau\", \"ngerampok\": \"merampok\", \"ngga\": \"tidak\", \"ngibul\": \"berbohong\", \"ngiler\": \"mau\", \"ngiri\": \"iri\", \"ngisiin\": \"mengisikan\", \"ngmng\": \"bicara\", \"ngomong\": \"bicara\", \"ngubek2\": \"mencari-cari\", \"ngurus\": \"mengurus\", \"nie\": \"ini\", \"nih\": \"ini\", \"niyh\": \"nih\", \"nmr\": \"nomor\", \"nntn\": \"nonton\", \"nobar\": \"nonton bareng\", \"np\": \"now playing\", \"ntar\": \"nanti\", \"ntn\": \"nonton\", \"numpuk\": \"bertumpuk\", \"nutupin\": \"menutupi\", \"nyari\": \"mencari\", \"nyekar\": \"menyekar\", \"nyicil\": \"mencicil\", \"nyoblos\": \"mencoblos\", \"nyokap\": \"ibu\", \"ogah\": \"tidak mau\", \"ol\": \"online\", \"ongkir\": \"ongkos kirim\", \"oot\": \"out of topic\", \"org2\": \"orang-orang\", \"ortu\": \"orang tua\", \"otda\": \"otonomi daerah\", \"otw\": \"on the way, sedang di jalan\", \"pacal\": \"pacar\", \"pake\": \"pakai\", \"pala\": \"kepala\", \"pansus\": \"panitia khusus\", \"parpol\": \"partai politik\", \"pasutri\": \"pasangan suami istri\", \"pd\": \"pada\", \"pede\": \"percaya diri\", \"pelatnas\": \"pemusatan latihan nasional\", \"pemda\": \"pemerintah daerah\", \"pemkot\": \"pemerintah kota\", \"pemred\": \"pemimpin redaksi\", \"penjas\": \"pendidikan jasmani\", \"perda\": \"peraturan daerah\", \"perhatiin\": \"perhatikan\", \"pesenan\": \"pesanan\", \"pgang\": \"pegang\", \"pi\": \"tapi\", \"pilkada\": \"pemilihan kepala daerah\", \"pisan\": \"sangat\", \"pk\": \"penjahat kelamin\", \"plg\": \"paling\", \"pmrnth\": \"pemerintah\", \"polantas\": \"polisi lalu lintas\", \"ponpes\": \"pondok pesantren\", \"pp\": \"pulang pergi\", \"prg\": \"pergi\", \"prnh\": \"pernah\", \"psen\": \"pesan\", \"pst\": \"pasti\", \"pswt\": \"pesawat\", \"pw\": \"posisi nyaman\", \"qmu\": \"kamu\", \"rakor\": \"rapat koordinasi\", \"ranmor\": \"kendaraan bermotor\", \"re\": \"reply\", \"ref\": \"referensi\", \"rehab\": \"rehabilitasi\", \"rempong\": \"sulit\", \"repp\": \"balas\", \"restik\": \"reserse narkotika\", \"rhs\": \"rahasia\", \"rmh\": \"rumah\", \"ru\": \"baru\", \"ruko\": \"rumah toko\", \"rusunawa\": \"rumah susun sewa\", \"ruz\": \"terus\", \"saia\": \"saya\", \"salting\": \"salah tingkah\", \"sampe\": \"sampai\", \"samsek\": \"sama sekali\", \"sapose\": \"siapa\", \"satpam\": \"satuan pengamanan\", \"sbb\": \"sebagai berikut\", \"sbh\": \"sebuah\", \"sbnrny\": \"sebenarnya\", \"scr\": \"secara\", \"sdgkn\": \"sedangkan\", \"sdkt\": \"sedikit\", \"se7\": \"setuju\", \"sebelas dua belas\": \"mirip\", \"sembako\": \"sembilan bahan pokok\", \"sempet\": \"sempat\", \"sendratari\": \"seni drama tari\", \"sgt\": \"sangat\", \"shg\": \"sehingga\", \"siech\": \"sih\", \"sikon\": \"situasi dan kondisi\", \"sinetron\": \"sinema elektronik\", \"siramin\": \"siramkan\", \"sj\": \"saja\", \"skalian\": \"sekalian\", \"sklh\": \"sekolah\", \"skt\": \"sakit\", \"slesai\": \"selesai\", \"sll\": \"selalu\", \"slma\": \"selama\", \"slsai\": \"selesai\", \"smpt\": \"sempat\", \"smw\": \"semua\", \"sndiri\": \"sendiri\", \"soljum\": \"sholat jumat\", \"songong\": \"sombong\", \"sory\": \"maaf\", \"sosek\": \"sosial-ekonomi\", \"sotoy\": \"sok tahu\", \"spa\": \"siapa\", \"sppa\": \"siapa\", \"spt\": \"seperti\", \"srtfkt\": \"sertifikat\", \"stiap\": \"setiap\", \"stlh\": \"setelah\", \"suk\": \"masuk\", \"sumpek\": \"sempit\", \"syg\": \"sayang\", \"t4\": \"tempat\", \"tajir\": \"kaya\", \"tau\": \"tahu\", \"taw\": \"tahu\", \"td\": \"tadi\", \"tdk\": \"tidak\", \"teh\": \"kakak perempuan\", \"telat\": \"terlambat\", \"telmi\": \"telat berpikir\", \"temen\": \"teman\", \"tengil\": \"menyebalkan\", \"tepar\": \"terkapar\", \"tggu\": \"tunggu\", \"tgu\": \"tunggu\", \"thankz\": \"terima kasih\", \"thn\": \"tahun\", \"tilang\": \"bukti pelanggaran\", \"tipiwan\": \"TvOne\", \"tks\": \"terima kasih\", \"tlp\": \"telepon\", \"tls\": \"tulis\", \"tmbah\": \"tambah\", \"tmen2\": \"teman-teman\", \"tmpah\": \"tumpah\", \"tmpt\": \"tempat\", \"tngu\": \"tunggu\", \"tnyta\": \"ternyata\", \"tokai\": \"tai\", \"toserba\": \"toko serba ada\", \"tpi\": \"tapi\", \"trdhulu\": \"terdahulu\", \"trima\": \"terima kasih\", \"trm\": \"terima\", \"trs\": \"terus\", \"trutama\": \"terutama\", \"ts\": \"penulis\", \"tst\": \"tahu sama tahu\", \"ttg\": \"tentang\", \"tuch\": \"tuh\", \"tuir\": \"tua\", \"tw\": \"tahu\", \"u\": \"kamu\", \"ud\": \"sudah\", \"udah\": \"sudah\", \"ujg\": \"ujung\", \"ul\": \"ulangan\", \"unyu\": \"lucu\", \"uplot\": \"unggah\", \"urang\": \"saya\", \"usah\": \"perlu\", \"utk\": \"untuk\", \"valas\": \"valuta asing\", \"w/\": \"dengan\", \"wadir\": \"wakil direktur\", \"wamil\": \"wajib militer\", \"warkop\": \"warung kopi\", \"warteg\": \"warung tegal\", \"wat\": \"buat\", \"wkt\": \"waktu\", \"wtf\": \"what the fuck\", \"xixixi\": \"tertawa\", \"ya\": \"iya\", \"yap\": \"iya\", \"yaudah\": \"ya sudah\", \"yawdah\": \"ya sudah\", \"yg\": \"yang\", \"yl\": \"yang lain\", \"yo\": \"iya\", \"yowes\": \"ya sudah\", \"yup\": \"iya\", \"7an\": \"tujuan\", \"ababil\": \"abg labil\", \"acc\": \"accord\", \"adlah\": \"adalah\", \"adoh\": \"aduh\", \"aha\": \"tertawa\", \"aing\": \"saya\", \"aja\": \"saja\", \"ajj\": \"saja\", \"aka\": \"dikenal juga sebagai\", \"akko\": \"aku\", \"akku\": \"aku\", \"akyu\": \"aku\", \"aljasa\": \"asal jadi saja\", \"ama\": \"sama\", \"ambl\": \"ambil\", \"anjir\": \"anjing\", \"ank\": \"anak\", \"ap\": \"apa\", \"apaan\": \"apa\", \"ape\": \"apa\", \"aplot\": \"unggah\", \"apva\": \"apa\", \"aqu\": \"aku\", \"asap\": \"sesegera mungkin\", \"aseek\": \"asyik\", \"asek\": \"asyik\", \"aseknya\": \"asyiknya\", \"asoy\": \"asyik\", \"astrojim\": \"astagfirullahaladzim\", \"ath\": \"kalau begitu\", \"atuh\": \"kalau begitu\", \"ava\": \"avatar\", \"aws\": \"awas\", \"ayang\": \"sayang\", \"ayok\": \"ayo\", \"bacot\": \"banyak bicara\", \"bales\": \"balas\", \"bangdes\": \"pembangunan desa\", \"bangkotan\": \"tua\", \"banpres\": \"bantuan presiden\", \"bansarkas\": \"bantuan sarana kesehatan\", \"bazis\": \"badan amal, zakat, infak, dan sedekah\", \"bcoz\": \"karena\", \"beb\": \"sayang\", \"bejibun\": \"banyak\", \"belom\": \"belum\", \"bener\": \"benar\", \"ber2\": \"berdua\", \"berdikari\": \"berdiri di atas kaki sendiri\", \"bet\": \"banget\", \"beti\": \"beda tipis\", \"beut\": \"banget\", \"bgd\": \"banget\", \"bgs\": \"bagus\", \"bhubu\": \"tidur\", \"bimbuluh\": \"bimbingan dan penyuluhan\", \"bisi\": \"kalau-kalau\", \"bkn\": \"bukan\", \"bl\": \"beli\", \"blg\": \"bilang\", \"blm\": \"belum\", \"bls\": \"balas\", \"bnchi\": \"benci\", \"bngung\": \"bingung\", \"bnyk\": \"banyak\", \"bohay\": \"badan aduhai\", \"bokep\": \"porno\", \"bokin\": \"pacar\", \"bole\": \"boleh\", \"bolot\": \"bodoh\", \"bonyok\": \"ayah ibu\", \"bpk\": \"bapak\", \"brb\": \"segera kembali\", \"brngkt\": \"berangkat\", \"brp\": \"berapa\", \"brur\": \"saudara laki-laki\", \"bsa\": \"bisa\", \"bsk\": \"besok\", \"bu_bu\": \"tidur\", \"bubarin\": \"bubarkan\", \"buber\": \"buka bersama\", \"bujubune\": \"luar biasa\", \"buser\": \"buru sergap\", \"bwhn\": \"bawahan\", \"byar\": \"bayar\", \"byr\": \"bayar\", \"c8\": \"chat\", \"cabut\": \"pergi\", \"caem\": \"cakep\", \"cama-cama\": \"sama-sama\", \"cangcut\": \"celana dalam\", \"cape\": \"capek\", \"caur\": \"jelek\", \"cekak\": \"tidak ada uang\", \"cekidot\": \"coba lihat\", \"cemplungin\": \"cemplungkan\", \"ceper\": \"pendek\", \"ceu\": \"kakak perempuan\", \"cewe\": \"cewek\", \"cibuk\": \"sibuk\", \"cin\": \"cinta\", \"ciye\": \"cie\", \"ckck\": \"ck\", \"clbk\": \"cinta lama bersemi kembali\", \"cmpr\": \"campur\", \"cnenk\": \"senang\", \"congor\": \"mulut\", \"cow\": \"cowok\", \"coz\": \"karena\", \"cpa\": \"siapa\", \"gokil\": \"gila\", \"gombal\": \"suka merayu\", \"gpl\": \"tidak pakai lama\", \"gpp\": \"tidak apa-apa\", \"gretong\": \"gratis\", \"gt\": \"begitu\", \"gtw\": \"tidak tahu\", \"gue\": \"saya\", \"guys\": \"teman-teman\", \"gws\": \"cepat sembuh\", \"haghaghag\": \"tertawa\", \"hakhak\": \"tertawa\", \"handak\": \"bahan peledak\", \"hansip\": \"pertahanan sipil\", \"hellow\": \"halo\", \"helow\": \"halo\", \"hi\": \"hai\", \"hlng\": \"hilang\", \"hnya\": \"hanya\", \"houm\": \"rumah\", \"hrs\": \"harus\", \"hubad\": \"hubungan angkatan darat\", \"hubla\": \"perhubungan laut\", \"huft\": \"mengeluh\", \"humas\": \"hubungan masyarakat\", \"idk\": \"saya tidak tahu\", \"ilfeel\": \"tidak suka\", \"imba\": \"jago sekali\", \"imoet\": \"imut\", \"info\": \"informasi\", \"itung\": \"hitung\", \"isengin\": \"bercanda\", \"iyala\": \"iya lah\", \"iyo\": \"iya\", \"jablay\": \"jarang dibelai\", \"jadul\": \"jaman dulu\", \"jancuk\": \"anjing\", \"jd\": \"jadi\", \"jdikan\": \"jadikan\", \"jg\": \"juga\", \"jgn\": \"jangan\", \"jijay\": \"jijik\", \"jkt\": \"jakarta\", \"jnj\": \"janji\", \"jth\": \"jatuh\", \"jurdil\": \"jujur adil\", \"jwb\": \"jawab\", \"ka\": \"kakak\", \"kabag\": \"kepala bagian\", \"kacian\": \"kasihan\", \"kadit\": \"kepala direktorat\", \"kaga\": \"tidak\", \"kaka\": \"kakak\", \"kamtib\": \"keamanan dan ketertiban\", \"kamuh\": \"kamu\", \"kamyu\": \"kamu\", \"kapt\": \"kapten\", \"kasat\": \"kepala satuan\", \"kasubbid\": \"kepala subbidang\", \"kau\": \"kamu\", \"kbar\": \"kabar\", \"kcian\": \"kasihan\", \"keburu\": \"terlanjur\", \"kedubes\": \"kedutaan besar\", \"kek\": \"seperti\", \"keknya\": \"kayaknya\", \"keliatan\": \"kelihatan\", \"keneh\": \"masih\", \"kepikiran\": \"terpikirkan\", \"kepo\": \"mau tahu urusan orang\", \"kere\": \"tidak punya uang\", \"kesian\": \"kasihan\", \"ketauan\": \"ketahuan\", \"keukeuh\": \"keras kepala\", \"khan\": \"kan\", \"kibus\": \"kaki busuk\", \"kk\": \"kakak\", \"klian\": \"kalian\", \"klo\": \"kalau\", \"kluarga\": \"keluarga\", \"klwrga\": \"keluarga\", \"kmari\": \"kemari\", \"kmpus\": \"kampus\", \"kn\": \"kan\", \"knl\": \"kenal\", \"knpa\": \"kenapa\", \"kog\": \"kok\", \"kompi\": \"komputer\", \"komtiong\": \"komunis Tiongkok\", \"konjen\": \"konsulat jenderal\", \"koq\": \"kok\", \"kpd\": \"kepada\", \"kptsan\": \"keputusan\", \"krik\": \"garing\", \"krn\": \"karena\", \"ktauan\": \"ketahuan\", \"ktny\": \"katanya\", \"kudu\": \"harus\", \"kuq\": \"kok\", \"ky\": \"seperti\", \"kykny\": \"kayanya\", \"laka\": \"kecelakaan\", \"lambreta\": \"lambat\", \"lansia\": \"lanjut usia\", \"lapas\": \"lembaga pemasyarakatan\", \"lbur\": \"libur\", \"lekong\": \"laki-laki\", \"lg\": \"lagi\", \"lgkp\": \"lengkap\", \"lht\": \"lihat\", \"linmas\": \"perlindungan masyarakat\", \"lmyan\": \"lumayan\", \"lngkp\": \"lengkap\", \"loch\": \"loh\", \"lol\": \"tertawa\", \"lom\": \"belum\", \"loupz\": \"cinta\", \"lowh\": \"kamu\", \"lu\": \"kamu\", \"luchu\": \"lucu\", \"luff\": \"cinta\", \"luph\": \"cinta\", \"lw\": \"kamu\", \"lwt\": \"lewat\", \"maaciw\": \"terima kasih\", \"mabes\": \"markas besar\", \"macem-macem\": \"macam-macam\", \"madesu\": \"masa depan suram\", \"maen\": \"main\", \"mahatma\": \"maju sehat bersama\", \"mak\": \"ibu\", \"makasih\": \"terima kasih\", \"malah\": \"bahkan\", \"malu2in\": \"memalukan\", \"mamz\": \"makan\", \"manies\": \"manis\", \"mantep\": \"mantap\", \"markus\": \"makelar kasus\", \"mba\": \"mbak\", \"mending\": \"lebih baik\", \"mgkn\": \"mungkin\", \"mhn\": \"mohon\", \"miker\": \"minuman keras\", \"milis\": \"mailing list\", \"mksd\": \"maksud\", \"mls\": \"malas\", \"mnt\": \"minta\", \"moge\": \"motor gede\", \"mokat\": \"mati\", \"mosok\": \"masa\", \"msh\": \"masih\", \"mskpn\": \"meskipun\", \"msng2\": \"masing-masing\", \"muahal\": \"mahal\", \"muker\": \"musyawarah kerja\", \"mumet\": \"pusing\", \"muna\": \"munafik\", \"munaslub\": \"musyawarah nasional luar biasa\", \"musda\": \"musyawarah daerah\", \"muup\": \"maaf\", \"muuv\": \"maaf\", \"nal\": \"kenal\", \"nangis\": \"menangis\", \"naon\": \"apa\", \"napol\": \"narapidana politik\", \"naq\": \"anak\", \"narsis\": \"bangga pada diri sendiri\", \"nax\": \"anak\", \"ndak\": \"tidak\", \"ndut\": \"gendut\", \"nekolim\": \"neokolonialisme\", \"nelfon\": \"menelepon\", \"ngabis2in\": \"menghabiskan\", \"ngakak\": \"tertawa\", \"ngambek\": \"marah\", \"ngampus\": \"pergi ke kampus\", \"ngantri\": \"mengantri\", \"ngapain\": \"sedang apa\", \"ngaruh\": \"berpengaruh\", \"ngawur\": \"berbicara sembarangan\", \"ngeceng\": \"kumpul bareng-bareng\", \"ngeh\": \"sadar\", \"ngekos\": \"tinggal di kos\", \"ngelamar\": \"melamar\", \"ngeliat\": \"melihat\", \"ngemeng\": \"bicara terus-terusan\", \"ngerti\": \"mengerti\", \"nggak\": \"tidak\", \"ngikut\": \"ikut\", \"nginep\": \"menginap\", \"ngisi\": \"mengisi\", \"ngmg\": \"bicara\", \"ngocol\": \"lucu\", \"ngomongin\": \"membicarakan\", \"ngumpul\": \"berkumpul\", \"ni\": \"ini\", \"nyasar\": \"tersesat\", \"nyariin\": \"mencari\", \"nyiapin\": \"mempersiapkan\", \"nyiram\": \"menyiram\", \"nyok\": \"ayo\", \"o/\": \"oleh\", \"ok\": \"ok\", \"priksa\": \"periksa\", \"pro\": \"profesional\", \"psn\": \"pesan\", \"psti\": \"pasti\", \"puanas\": \"panas\", \"qmo\": \"kamu\", \"qt\": \"kita\", \"rame\": \"ramai\", \"raskin\": \"rakyat miskin\", \"red\": \"redaksi\", \"reg\": \"register\", \"rejeki\": \"rezeki\", \"renstra\": \"rencana strategis\", \"reskrim\": \"reserse kriminal\", \"sni\": \"sini\", \"somse\": \"sombong sekali\", \"sorry\": \"maaf\", \"sosbud\": \"sosial-budaya\", \"sospol\": \"sosial-politik\", \"sowry\": \"maaf\", \"spd\": \"sepeda\", \"sprti\": \"seperti\", \"spy\": \"supaya\", \"stelah\": \"setelah\", \"subbag\": \"subbagian\", \"sumbangin\": \"sumbangkan\", \"sy\": \"saya\", \"syp\": \"siapa\", \"tabanas\": \"tabungan pembangunan nasional\", \"tar\": \"nanti\", \"taun\": \"tahun\", \"tawh\": \"tahu\", \"tdi\": \"tadi\", \"te2p\": \"tetap\", \"tekor\": \"rugi\", \"telkom\": \"telekomunikasi\", \"telp\": \"telepon\", \"temen2\": \"teman-teman\", \"tengok\": \"menjenguk\", \"terbitin\": \"terbitkan\", \"tgl\": \"tanggal\", \"thanks\": \"terima kasih\", \"thd\": \"terhadap\", \"thx\": \"terima kasih\", \"tipi\": \"TV\", \"tkg\": \"tukang\", \"tll\": \"terlalu\", \"tlpn\": \"telepon\", \"tman\": \"teman\", \"tmbh\": \"tambah\", \"tmn2\": \"teman-teman\", \"tmph\": \"tumpah\", \"tnda\": \"tanda\", \"tnh\": \"tanah\", \"togel\": \"toto gelap\", \"tp\": \"tapi\", \"tq\": \"terima kasih\", \"trgntg\": \"tergantung\", \"trims\": \"terima kasih\", \"cb\": \"coba\", \"y\": \"ya\", \"munfik\": \"munafik\", \"reklamuk\": \"reklamasi\", \"sma\": \"sama\", \"tren\": \"trend\", \"ngehe\": \"kesal\", \"mz\": \"mas\", \"analisise\": \"analisis\", \"sadaar\": \"sadar\", \"sept\": \"september\", \"nmenarik\": \"menarik\", \"zonk\": \"bodoh\", \"rights\": \"benar\", \"simiskin\": \"miskin\", \"ngumpet\": \"sembunyi\", \"hardcore\": \"keras\", \"akhirx\": \"akhirnya\", \"solve\": \"solusi\", \"watuk\": \"batuk\", \"ngebully\": \"intimidasi\", \"masy\": \"masyarakat\", \"still\": \"masih\", \"tauk\": \"tahu\", \"mbual\": \"bual\", \"tioghoa\": \"tionghoa\", \"ngentotin\": \"senggama\", \"kentot\": \"senggama\", \"faktakta\": \"fakta\", \"sohib\": \"teman\", \"rubahnn\": \"rubah\", \"trlalu\": \"terlalu\", \"nyela\": \"cela\", \"heters\": \"pembenci\", \"nyembah\": \"sembah\", \"most\": \"paling\", \"ikon\": \"lambang\", \"light\": \"terang\", \"pndukung\": \"pendukung\", \"setting\": \"atur\", \"seting\": \"akting\", \"next\": \"lanjut\", \"waspadalah\": \"waspada\", \"gantengsaya\": \"ganteng\", \"parte\": \"partai\", \"nyerang\": \"serang\", \"nipu\": \"tipu\", \"ktipu\": \"tipu\", \"jentelmen\": \"berani\", \"buangbuang\": \"buang\", \"tsangka\": \"tersangka\", \"kurng\": \"kurang\", \"ista\": \"nista\", \"less\": \"kurang\", \"koar\": \"teriak\", \"paranoid\": \"takut\", \"problem\": \"masalah\", \"tahi\": \"kotoran\", \"tirani\": \"tiran\", \"tilep\": \"tilap\", \"happy\": \"bahagia\", \"tak\": \"tidak\", \"penertiban\": \"tertib\", \"uasai\": \"kuasa\", \"mnolak\": \"tolak\", \"trending\": \"trend\", \"taik\": \"tahi\", \"wkwkkw\": \"tertawa\", \"ahokncc\": \"ahok\", \"istaa\": \"nista\", \"benarjujur\": \"jujur\", \"mgkin\": \"mungkin\"}\n",
        "\n",
        "# Fungsi ini bertugas untuk memperbaiki kata-kata slang\n",
        "\n",
        "def fix_slangwords(text):\n",
        "    words = text.split()\n",
        "    fixed_words = []\n",
        "\n",
        "    for word in words:\n",
        "        word_lower = word.lower()  # Mengubah kata menjadi lowercase satu kali\n",
        "        fixed_word = slangwords.get(word_lower, word)  # Mencari kata yang diperbaiki dari dictionary, jika tidak ada, biarkan kata tetap\n",
        "        fixed_words.append(fixed_word) # Menambahkan kata yang sudah diperbaiki ke dalam fixed_words\n",
        "\n",
        "    fixed_text = ' '.join(fixed_words)  # Menggabungkan kata-kata yang sudah diperbaiki menjadi sebuah kalimat\n",
        "    return fixed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBbV-tYbe1M0"
      },
      "outputs": [],
      "source": [
        "# Membersihkan teks dan menyimpannya di kolom 'text_clean'\n",
        "df_reviews['text_clean'] = df_reviews['Review'].apply(clean_and_casefold)\n",
        "\n",
        "# Menghapus kata-kata slang dengan kata-kata standar\n",
        "df_reviews['text_slangwords'] = df_reviews['text_clean'].apply(fix_slangwords)\n",
        "\n",
        "# Memecah teks menjadi token (kata-kata) dan menghapus stopwords\n",
        "df_reviews['text_tokenizingText'] = df_reviews['text_slangwords'].apply(tokenize_and_filter)\n",
        "\n",
        "# Menggabungkan token-token menjadi kalimat\n",
        "df_reviews['text_akhir'] = df_reviews['text_tokenizingText'].apply(to_sentence)\n",
        "\n",
        "# Jika perlu, bisa juga melakukan stemming dan menyimpannya (dikarenakan dataset besar, proses steming tidak dilakukan)\n",
        "#df_reviews['text_stemmed'] = df_reviews['text_tokenizingText'].apply(stem_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pelabelan"
      ],
      "metadata": {
        "id": "FgL62lWSNFq2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmWDUSMhKrD-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49e02f41-6745-44d2-cc96-1aca84c31ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contoh data dari lexicon positif: [('hai', 3), ('merekam', 2), ('ekstensif', 3), ('paripurna', 1), ('detail', 2)]\n",
            "Contoh data dari lexicon negatif: [('putus tali gantung', -2), ('gelebah', -2), ('gobar hati', -2), ('tersentuh (perasaan)', -1), ('isak', -5)]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import csv\n",
        "from io import StringIO\n",
        "\n",
        "# Fungsi untuk memuat data lexicon dari URL CSV\n",
        "def load_lexicon_from_url(url):\n",
        "    lexicon = dict()\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Menggunakan StringIO untuk membaca CSV dari string yang diterima\n",
        "        reader = csv.reader(StringIO(response.text), delimiter=',')\n",
        "\n",
        "        # Membaca setiap baris dalam CSV dan menambahkannya ke kamus\n",
        "        for row in reader:\n",
        "            lexicon[row[0]] = int(row[1])  # Menambahkan kata dan skornya ke kamus\n",
        "    else:\n",
        "        print(f\"Failed to fetch data from {url}\")\n",
        "\n",
        "    return lexicon\n",
        "\n",
        "# Memuat data kamus positif dan negatif menggunakan fungsi di atas\n",
        "lexicon_positive = load_lexicon_from_url('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')\n",
        "lexicon_negative = load_lexicon_from_url('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')\n",
        "\n",
        "# Menampilkan beberapa kata dan skor dari lexicon positif dan negatif\n",
        "print(\"Contoh data dari lexicon positif:\", list(lexicon_positive.items())[:5])\n",
        "print(\"Contoh data dari lexicon negatif:\", list(lexicon_negative.items())[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tThAkTMyvINL"
      },
      "outputs": [],
      "source": [
        "# Fungsi untuk melakukan analisis sentimen berdasarkan lexicon positif dan negatif dalam teks bahasa Indonesia\n",
        "\n",
        "def sentiment_analysis_lexicon_indonesia(text):\n",
        "    score = 0  # Inisialisasi skor sentimen ke 0\n",
        "\n",
        "    # Gabungkan lexicon_positive dan lexicon_negative dalam satu loop untuk efisiensi\n",
        "    for word in text:\n",
        "        if word in lexicon_positive:\n",
        "            score += lexicon_positive[word]  # Tambahkan skor dari lexicon positif\n",
        "        elif word in lexicon_negative:\n",
        "            score += lexicon_negative[word]  # Kurangi skor dari lexicon negatif\n",
        "\n",
        "    # Tentukan polaritas berdasarkan skor\n",
        "    if score > 0:\n",
        "        polarity = 'positive'  # Skor lebih besar dari 0 -> positif\n",
        "    elif score < 0:\n",
        "        polarity = 'negative'  # Skor kurang dari 0 -> negatif\n",
        "    else:\n",
        "        polarity = 'netral'\n",
        "\n",
        "    return score, polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYpUq2rWzwgb",
        "outputId": "f0f9d4e2-fcd6-4f2c-ff97-3497753fd987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "polarity\n",
            "positive    62540\n",
            "negative    27538\n",
            "netral       9922\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Menerapkan analisis sentimen dan memecah hasilnya menjadi dua kolom\n",
        "df_reviews[['polarity_score', 'polarity']] = df_reviews['text_tokenizingText'].apply(lambda text: pd.Series(sentiment_analysis_lexicon_indonesia(text)))\n",
        "\n",
        "# Menampilkan distribusi polaritas berdasarkan jumlah masing-masing kategori\n",
        "print(df_reviews['polarity'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ekstraksi Fitur And Model 3 Skema"
      ],
      "metadata": {
        "id": "6BjV6jnBLaVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TD-IDF dan Losgistic Regression (80:20)"
      ],
      "metadata": {
        "id": "yaIspFGUOqcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Pisahkan data menjadi fitur (tweet) dan label (sentimen)\n",
        "X = df_reviews['text_akhir']\n",
        "y = df_reviews['polarity']\n",
        "\n",
        "# Ekstraksi fitur dengan TF-IDF, menambahkan ngram_range dan meningkatkan max_features\n",
        "tfidf = TfidfVectorizer(max_features=8000,  # Menambah jumlah fitur\n",
        "                        min_df=1,           # Menurunkan min_df agar lebih banyak kata yang digunakan\n",
        "                        max_df=0.8,         # Menurunkan max_df agar kata yang terlalu sering tidak dominan\n",
        "                        ngram_range=(1,2),\n",
        "                        stop_words=stopwords.words('indonesian')) # Menambahkan bigram (1, 2)\n",
        "\n",
        "X_tfidf = tfidf.fit_transform(X)\n",
        "\n",
        "# Bagi data menjadi data latih dan data uji\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Menampilkan beberapa fitur yang diambil (kata-kata yang digunakan untuk representasi TF-IDF)\n",
        "features = tfidf.get_feature_names_out()\n",
        "print(\"Fitur yang diekstraksi (kata-kata dan n-gram):\")\n",
        "print(features[:20])  # Menampilkan 20 fitur pertama\n",
        "\n",
        "# Menampilkan matriks TF-IDF sebagai DataFrame untuk melihat nilai-nilai TF-IDF\n",
        "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=features)\n",
        "\n",
        "# Menampilkan beberapa baris pertama dari matriks TF-IDF\n",
        "print(\"\\nBeberapa baris pertama dari matriks TF-IDF:\")\n",
        "print(df_tfidf.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dvh56eqV2Qwl",
        "outputId": "cf2ac317-2857-4ba4-d4c0-47fae5b118d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'sekurang', 'setidak', 'tama', 'tidaknya'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitur yang diekstraksi (kata-kata dan n-gram):\n",
            "['abang' 'abdet' 'abu' 'acara' 'adaa' 'adain' 'adain fitur' 'adakan' 'add'\n",
            " 'ade' 'adek' 'adik' 'adil' 'admin' 'admin capcut' 'admin tolong'\n",
            " 'adminnya' 'ads' 'aduh' 'aduhh']\n",
            "\n",
            "Beberapa baris pertama dari matriks TF-IDF:\n",
            "   abang  abdet  abu  acara  adaa  adain  adain fitur  adakan  add  ade  ...  \\\n",
            "0    0.0    0.0  0.0    0.0   0.0    0.0          0.0     0.0  0.0  0.0  ...   \n",
            "1    0.0    0.0  0.0    0.0   0.0    0.0          0.0     0.0  0.0  0.0  ...   \n",
            "2    0.0    0.0  0.0    0.0   0.0    0.0          0.0     0.0  0.0  0.0  ...   \n",
            "3    0.0    0.0  0.0    0.0   0.0    0.0          0.0     0.0  0.0  0.0  ...   \n",
            "4    0.0    0.0  0.0    0.0   0.0    0.0          0.0     0.0  0.0  0.0  ...   \n",
            "\n",
            "   𝙣𝙮𝙖  𝚊𝚔𝚞  𝚊𝚙𝚕𝚒𝚔𝚊𝚜𝚒  𝚋𝚊𝚐𝚞𝚜  𝚋𝚊𝚗𝚐𝚎𝚝  𝚋𝚒𝚜𝚊   𝚍𝚒  𝚒𝚗𝚒  𝚗𝚢𝚊  𝚜𝚊𝚢𝚊  \n",
            "0  0.0  0.0       0.0    0.0     0.0   0.0  0.0  0.0  0.0   0.0  \n",
            "1  0.0  0.0       0.0    0.0     0.0   0.0  0.0  0.0  0.0   0.0  \n",
            "2  0.0  0.0       0.0    0.0     0.0   0.0  0.0  0.0  0.0   0.0  \n",
            "3  0.0  0.0       0.0    0.0     0.0   0.0  0.0  0.0  0.0   0.0  \n",
            "4  0.0  0.0       0.0    0.0     0.0   0.0  0.0  0.0  0.0   0.0  \n",
            "\n",
            "[5 rows x 8000 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Membuat objek model Logistic Regression\n",
        "logistic_regression = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Lakukan GridSearchCV untuk optimasi hyperparameter\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],  # C adalah parameter regulasi (penalti)\n",
        "    'solver': ['liblinear', 'newton-cg', 'lbfgs', 'saga'],  # Coba solver yang berbeda\n",
        "    'penalty': ['l2']  # L2 regularization\n",
        "}\n",
        "\n",
        "# Menggunakan GridSearchCV untuk mencari parameter terbaik\n",
        "grid_search = GridSearchCV(logistic_regression, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Menampilkan parameter terbaik dari GridSearchCV\n",
        "print(\"Parameter terbaik adalah: \", grid_search.best_params_)\n",
        "\n",
        "# Menggunakan model dengan hyperparameter terbaik\n",
        "best_logistic_regression = grid_search.best_estimator_\n",
        "\n",
        "# Prediksi sentimen pada data pelatihan dan data uji\n",
        "y_pred_train_lr = best_logistic_regression.predict(X_train)\n",
        "y_pred_test_lr = best_logistic_regression.predict(X_test)\n",
        "\n",
        "# Evaluasi akurasi model Logistic Regression pada data pelatihan\n",
        "accuracy_train_lr = accuracy_score(y_pred_train_lr, y_train)\n",
        "\n",
        "# Evaluasi akurasi model Logistic Regression pada data uji\n",
        "accuracy_test_lr = accuracy_score(y_pred_test_lr, y_test)\n",
        "\n",
        "# Menampilkan akurasi\n",
        "print('Logistic Regression - accuracy_train:', accuracy_train_lr)\n",
        "print('Logistic Regression - accuracy_test:', accuracy_test_lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-dSfYtHJIob",
        "outputId": "dca2d506-1cf5-4265-d074-d339a9c64950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Parameter terbaik adalah:  {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "Logistic Regression - accuracy_train: 0.9565\n",
            "Logistic Regression - accuracy_test: 0.9253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input kalimat baru dari pengguna\n",
        "kalimat_baru = input(\"Masukkan kalimat baru: \")\n",
        "\n",
        "# Melakukan preprocessing pada kalimat baru\n",
        "kalimat_baru_clean_and_casefold = clean_and_casefold(kalimat_baru)\n",
        "kalimat_baru_fix_slangwords = fix_slangwords(kalimat_baru_clean_and_casefold)\n",
        "kalimat_baru_tokenize_and_filter = tokenize_and_filter(kalimat_baru_fix_slangwords)\n",
        "kalimat_baru_final = to_sentence(kalimat_baru_tokenize_and_filter)\n",
        "\n",
        "# Menggunakan objek tfidf yang sudah di-fit dari pelatihan sebelumnya\n",
        "X_kalimat_baru = tfidf.transform([kalimat_baru_final])\n",
        "\n",
        "model = best_logistic_regression\n",
        "\n",
        "# Memperoleh prediksi sentimen kalimat baru\n",
        "prediksi_sentimen = model.predict(X_kalimat_baru.toarray())\n",
        "\n",
        "# Menampilkan hasil prediksi\n",
        "if prediksi_sentimen[0] == 'positive':\n",
        "    print(\"Sentimen kalimat baru adalah positif.\")\n",
        "elif prediksi_sentimen[0] == 'negative':\n",
        "    print(\"Sentimen kalimat baru adalah negatif.\")\n",
        "else:\n",
        "    print(\"Sentimen kalimat baru adalah netral.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oaeioLiw_QZ",
        "outputId": "98d430f8-9c24-4b8f-96ca-afab078d52aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masukkan kalimat baru: aplikasi ini jelek dan buruk\n",
            "Sentimen kalimat baru adalah negatif.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word2Vec dan LSTM (80:20)"
      ],
      "metadata": {
        "id": "mZzA8Ml4QQ_O"
      }
    },
    {
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Tokenisasi teks\n",
        "df_reviews['tokens'] = df_reviews['text_akhir'].apply(nltk.word_tokenize)\n",
        "\n",
        "# Buat dan latih model Word2Vec (Untuk mendapatkan representasi vektor kata)\n",
        "model_w2v = Word2Vec(sentences=df_reviews['tokens'].tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
        "model_w2v.save(\"word2vec.model\")  # Simpan model Word2Vec yang telah dilatih\n",
        "\n",
        "# Buat kamus kata dan ubah teks menjadi urutan angka\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_reviews['text_akhir'].tolist())\n",
        "sequences = tokenizer.texts_to_sequences(df_reviews['text_akhir'].tolist())\n",
        "\n",
        "# Padding urutan agar memiliki panjang yang sama\n",
        "max_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Buat matriks embedding\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Ukuran kosa kata (jumlah kata unik)\n",
        "embedding_matrix = np.zeros((vocab_size, 100)) # Membuat matriks embedding dengan ukuran (vocab_size, 100) untuk vektor 100 dimensi\n",
        "\n",
        "# Mengisi matriks embedding dengan representasi vektor kata dari model Word2Vec\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in model_w2v.wv:\n",
        "        embedding_matrix[i] = model_w2v.wv[word]\n",
        "\n",
        "# Menentukan variabel input (X) dan label (y)\n",
        "X = padded_sequences\n",
        "y = df_reviews['polarity'] # Menggunakan kolom 'polarity' sebagai label target\n",
        "\n",
        "# Bagi data menjadi data latih dan data uji\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Menampilkan kata-kata (features) yang ada dalam tokenizer\n",
        "print(\"Kata-kata yang digunakan dalam Tokenizer:\")\n",
        "print(list(tokenizer.word_index.keys())[:20])  # Menampilkan 20 kata pertama\n",
        "\n",
        "# Menampilkan vektor kata (embedding) untuk beberapa kata tertentu\n",
        "words_to_check = ['positif', 'negatif', 'pelit', 'buruk']  # Misalnya, beberapa kata yang ingin kita periksa\n",
        "print(\"\\nRepresentasi vektor kata untuk beberapa kata:\")\n",
        "for word in words_to_check:\n",
        "    if word in model_w2v.wv:\n",
        "        print(f\"{word}: {model_w2v.wv[word]}\")  # Menampilkan vektor kata untuk setiap kata\n",
        "\n",
        "# Menampilkan beberapa baris pertama dari matriks embedding\n",
        "print(\"\\nBeberapa baris pertama dari matriks embedding:\")\n",
        "df_embedding = pd.DataFrame(embedding_matrix[:10], columns=[f\"Dim_{i+1}\" for i in range(100)])  # Menampilkan 10 baris pertama\n",
        "print(df_embedding.head())"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i55liajJ2_oF",
        "outputId": "a0823bf4-b9d5-4439-8eef-aab4f20f3e96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kata-kata yang digunakan dalam Tokenizer:\n",
            "['bagus', 'capcut', 'aplikasi', 'banget', 'iklan', 'apk', 'tolong', 'profesional', 'ngedit', 'suka', 'video', 'pakai', 'download', 'kasih', 'pas', 'template', 'edit', 'gk', 'update', 'fitur']\n",
            "\n",
            "Representasi vektor kata untuk beberapa kata:\n",
            "positif: [ 0.09376886 -0.07216099  0.19129121  0.21398048 -0.17632471 -0.22302707\n",
            "  0.02573552  0.25973526 -0.18230163  0.1129346  -0.21603636 -0.30789456\n",
            "  0.02154708  0.44215906  0.18209092 -0.07549606  0.1726135  -0.15700553\n",
            " -0.21275719 -0.43492976  0.1886847   0.43711945 -0.0132981   0.01134706\n",
            "  0.11364112  0.12094709 -0.11667144  0.00876291 -0.24585754  0.38252226\n",
            "  0.12109509  0.09045428  0.23202033 -0.22371462 -0.38420308  0.3741466\n",
            " -0.13187003 -0.3777108  -0.22300433 -0.517766    0.06697795 -0.4158111\n",
            "  0.15705223  0.01816021  0.21493146 -0.16126066 -0.38223737 -0.12853174\n",
            "  0.08822538  0.0969793  -0.21754721 -0.408771    0.0214332  -0.05760007\n",
            " -0.18805163  0.35923573  0.24434814  0.24739674 -0.06645859 -0.17652087\n",
            "  0.04824476  0.12498224 -0.11098146  0.10876636 -0.3336764   0.17021526\n",
            " -0.08482352  0.20887691 -0.29219216  0.14846563  0.06485596 -0.2009901\n",
            "  0.07254899 -0.10771095  0.36116415  0.12194169 -0.00621021  0.12151415\n",
            " -0.22216251 -0.17470568  0.05834921 -0.14068335 -0.29640013  0.4054847\n",
            "  0.21201327  0.36725578 -0.04325944  0.11631253  0.30620015 -0.23346883\n",
            "  0.18115588 -0.12940338 -0.35095844  0.09241807 -0.24143875  0.4283821\n",
            "  0.10826919 -0.4110941   0.1798799   0.032732  ]\n",
            "negatif: [ 0.00310814 -0.02562824  0.06685373  0.07026832 -0.08681485 -0.10418705\n",
            "  0.02004003  0.10157622 -0.07220522  0.03394429 -0.0903132  -0.10406173\n",
            " -0.02481707  0.09189146  0.088755    0.04773309 -0.02805819 -0.06450253\n",
            " -0.06120064 -0.1435192   0.05324621  0.14279476 -0.00078956  0.0113111\n",
            "  0.04826412  0.02747514  0.01497476  0.00221602 -0.10246632  0.11898082\n",
            "  0.05728848 -0.02369197  0.19190055 -0.0810107  -0.07802971  0.15006948\n",
            " -0.03158106 -0.10868414 -0.03840575 -0.19923867 -0.00034647 -0.1891637\n",
            "  0.05428553  0.02429128  0.11013958 -0.15623958 -0.15177439  0.00136364\n",
            "  0.01391425  0.04146693 -0.01674546 -0.08163366  0.05049707 -0.06527954\n",
            " -0.07795622  0.18968849  0.15742986  0.09889209 -0.05736095 -0.00850122\n",
            " -0.00190184  0.13906628 -0.06377958 -0.02475759 -0.10567585  0.06837977\n",
            "  0.00529328  0.13297701 -0.09775358  0.0054016  -0.04691449 -0.10900299\n",
            " -0.02567226 -0.0181454   0.1825768   0.07500831 -0.00210313  0.0466209\n",
            " -0.05179581 -0.07984985 -0.02618136 -0.01573789 -0.09341133  0.19883199\n",
            "  0.06098631  0.15180759  0.02846243  0.08562994  0.17263713 -0.04283043\n",
            "  0.02179143 -0.06124514 -0.09490623  0.08209866 -0.0667491   0.17115153\n",
            "  0.03497475 -0.18812159  0.03401992  0.00941467]\n",
            "pelit: [ 1.2398095  -0.29191536  0.63718176  0.18438804 -0.46446505 -0.67167443\n",
            "  0.7598526   0.10545219 -0.04212028 -0.04617247  0.39381734  0.81982386\n",
            " -0.2903029   0.16262569  0.2581545   0.532987    0.5014499  -0.2028277\n",
            "  0.3922754  -0.5574015  -0.1360375   0.73109686  0.01141596 -0.46922535\n",
            " -0.15987836 -0.06791987 -0.35629746 -0.09643632 -0.21000867  1.2678378\n",
            "  0.6138667  -0.92196006  0.90026104 -0.41405436 -0.5134809  -0.24586183\n",
            " -1.0594525  -1.1191924  -0.59378535 -1.347317    0.4698822  -0.06847592\n",
            "  0.53552055 -0.05057627  0.42720902 -0.04668246 -0.5976533  -0.12017292\n",
            " -0.45877004  0.623298   -0.9275373   0.2434724  -0.41551724 -0.13040349\n",
            " -0.42432275  0.7295607   0.0188103   0.2700553   0.23326604 -0.35438746\n",
            "  0.29729098 -0.4753906  -0.09560439  0.7966816  -0.40328702  1.4483328\n",
            " -1.1784962   0.2880562  -1.2408385  -0.33514044 -0.91191643  0.49259776\n",
            "  0.27913746 -0.77132577  1.1389264  -0.41594437 -0.74579406  0.15877494\n",
            " -1.1710887  -1.2362056  -0.5950655  -0.7210693  -1.5037035   0.92850626\n",
            "  1.0356939   0.2165986   0.73163754 -0.56052697  0.12129539 -1.31026\n",
            "  0.04251073 -0.7753643  -1.5576631   0.72924876 -0.39412102 -0.10041356\n",
            " -1.1664959   0.01483155 -0.2903151   0.1356009 ]\n",
            "buruk: [ 0.16289805  0.4441275   1.0277615  -0.16271536  0.1417653  -1.9662864\n",
            "  1.0023882   1.2513617   0.00852141 -0.31277964 -0.4641641  -0.5770538\n",
            "  0.2307305   0.5220356   0.41137195  0.19217499  0.37960795 -0.37765044\n",
            "  0.5178656  -1.0029113  -0.19921716 -0.17486128  0.25083467  0.00999532\n",
            " -0.9347578  -0.825798   -1.0924362   1.3733075  -0.7956871  -0.3364347\n",
            "  0.23345974 -0.14626206  0.6969088  -0.31292886 -1.1630492   0.1435609\n",
            "  0.62055635 -1.2024245  -0.28943947 -1.3292923  -0.15298016 -0.11614594\n",
            " -0.84340996 -0.32659566  0.20075893  0.47432354  0.37439618 -0.01396263\n",
            "  0.66584283  0.52217966 -0.3949012  -0.52382153 -0.08419427 -0.05687825\n",
            " -0.43672124  0.28470677 -0.2741523  -0.42387936 -0.11403441 -0.6928843\n",
            " -0.05885681 -0.39081272 -0.7481398   0.19159527 -1.4096297   0.999332\n",
            " -0.45514992  1.3544039  -0.82691133  0.7839693  -0.83301705  0.9242724\n",
            "  2.4456503   0.01706921  0.8527376   0.36805984 -1.0391963  -0.01970581\n",
            " -1.0282503  -0.4565002  -0.51503336 -0.93397754 -1.2225798   1.2928729\n",
            " -0.23198406 -0.65333074  0.3224391   0.11974575  0.52167594 -0.9675384\n",
            "  0.86230636 -0.04002449 -0.41766733 -0.34536207  1.1229295   0.31213993\n",
            " -0.1813292   0.30509788  0.74957496  0.05942766]\n",
            "\n",
            "Beberapa baris pertama dari matriks embedding:\n",
            "      Dim_1     Dim_2     Dim_3     Dim_4     Dim_5     Dim_6     Dim_7  \\\n",
            "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "1 -0.145824 -0.519571  2.250545  0.297444 -0.752348 -1.049527  0.123332   \n",
            "2  0.511933 -0.867179  1.202655  1.531848  0.608507 -1.274179 -0.245612   \n",
            "3 -0.002118 -0.501673  2.475588  0.437633  0.382184 -1.105815  0.812748   \n",
            "4  0.140464 -0.202186  1.865131  0.334754  0.259339 -1.214134 -0.701407   \n",
            "\n",
            "      Dim_8     Dim_9    Dim_10  ...    Dim_91    Dim_92    Dim_93    Dim_94  \\\n",
            "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "1  1.952597 -2.055496 -2.143724  ...  0.759856  1.107437  0.094267 -0.133080   \n",
            "2  0.942136 -1.296688 -1.805096  ...  0.620747 -0.005230 -0.179461 -0.246072   \n",
            "3  1.228985 -2.434993 -2.344569  ... -0.322298  0.522221 -0.219497 -0.337982   \n",
            "4  1.900109  0.225986 -2.498712  ... -0.180402  0.738422 -0.347874  0.185710   \n",
            "\n",
            "     Dim_95    Dim_96    Dim_97    Dim_98    Dim_99   Dim_100  \n",
            "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "1  2.565542  1.296149 -0.693255 -0.682097  1.816854 -0.074275  \n",
            "2  0.097759  1.388400 -0.350318  1.135646 -0.017202 -0.330631  \n",
            "3  1.101543  1.339195  0.263729 -0.544252  1.292053 -1.491150  \n",
            "4  1.093742  0.753573  0.000031 -1.215493  0.287934 -1.151319  \n",
            "\n",
            "[5 rows x 100 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Define the embedding dimension\n",
        "embedding_dim = 100\n",
        "\n",
        "# Membaut model LSTM dengan embedding dari Word2Vec\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=X_train.shape[1]))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Kompilasi Model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Definisikan Custom Callback untuk menghentikan pelatihan jika akurasi mencapai threshold tertentu\n",
        "class MyThresholdCallback(Callback):\n",
        "    def __init__(self, threshold):\n",
        "        super(MyThresholdCallback, self).__init__()\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        accuracy = logs[\"accuracy\"]\n",
        "        val_accuracy = logs[\"val_accuracy\"]\n",
        "        if accuracy >= self.threshold and val_accuracy >= self.threshold:\n",
        "            self.model.stop_training = True\n",
        "            print(f\"\\nPelatihan dihentikan pada epoch {epoch + 1} karena akurasi latihan dan validasi mencapai {self.threshold * 100:.2f}%\")\n",
        "\n",
        "# Inisialisasi Custom Callback dengan threshold 0.93\n",
        "my_callback = MyThresholdCallback(threshold=0.93)\n",
        "\n",
        "# Mengubah label menjadi representasi numerik menggunakan LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "# Latih model dengan data pelatihan\n",
        "history = model.fit(X_train, y_train_encoded, epochs=20, batch_size=64, validation_split=0.2, callbacks=[my_callback])\n",
        "\n",
        "# Menampilkan akurasi\n",
        "train_acc = history.history['accuracy'][-1]\n",
        "val_acc = history.history['val_accuracy'][-1]\n",
        "\n",
        "print(f'Akurasi Training: {train_acc*100:.2f}%')\n",
        "print(f'Akurasi Validasi: {val_acc*100:.2f}%')\n",
        "\n",
        "# Evaluasi model menggunakan data uji\n",
        "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy *  100:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd877f23-5a75-4f91-af31-b2cef6a442fe",
        "id": "bLFEspTz0LOj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 604ms/step - accuracy: 0.8105 - loss: 0.4845 - val_accuracy: 0.9262 - val_loss: 0.2067\n",
            "Epoch 2/20\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - accuracy: 0.9451 - loss: 0.1517\n",
            "Pelatihan dihentikan pada epoch 2 karena akurasi latihan dan validasi mencapai 93.00%\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m600s\u001b[0m 582ms/step - accuracy: 0.9451 - loss: 0.1517 - val_accuracy: 0.9344 - val_loss: 0.1901\n",
            "Akurasi Training: 94.62%\n",
            "Akurasi Validasi: 93.44%\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 113ms/step - accuracy: 0.9310 - loss: 0.2037\n",
            "Test Loss: 0.1989\n",
            "Test Accuracy: 93.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk preprocessing kalimat baru\n",
        "def preprocess_kalimat(kalimat_baru):\n",
        "    # Melakukan preprocessing pada kalimat baru\n",
        "    kalimat_baru_clean_and_casefold = clean_and_casefold(kalimat_baru)\n",
        "    kalimat_baru_fix_slangwords = fix_slangwords(kalimat_baru_clean_and_casefold)\n",
        "    kalimat_baru_tokenize_and_filter = tokenize_and_filter(kalimat_baru_fix_slangwords)\n",
        "    kalimat_baru_final = to_sentence(kalimat_baru_tokenize_and_filter)\n",
        "    return kalimat_baru_final\n",
        "\n",
        "# Input kalimat baru dari pengguna\n",
        "kalimat_baru = input(\"Masukkan kalimat baru: \")\n",
        "\n",
        "# Preprocessing kalimat baru\n",
        "kalimat_baru_processed = preprocess_kalimat(kalimat_baru)\n",
        "\n",
        "# Tokenisasi kalimat baru dengan tokenizer yang sudah dilatih sebelumnya\n",
        "sequences_baru = tokenizer.texts_to_sequences([kalimat_baru_processed])\n",
        "\n",
        "# Padding urutan kalimat baru agar memiliki panjang yang sama\n",
        "X_kalimat_baru = pad_sequences(sequences_baru, maxlen=X_train.shape[1], padding='post')\n",
        "\n",
        "# Menggunakan model untuk memprediksi sentimen\n",
        "prediksi_sentimen = model.predict(X_kalimat_baru)\n",
        "\n",
        "# Mengonversi prediksi (output) menjadi label\n",
        "predicted_class = np.argmax(prediksi_sentimen, axis=1)\n",
        "\n",
        "# Mendapatkan label asli (positif, negatif, netral)\n",
        "sentimen_labels = ['negatif', 'netral', 'positif']\n",
        "\n",
        "# Menampilkan hasil prediksi\n",
        "print(f\"Sentimen kalimat baru adalah {sentimen_labels[predicted_class[0]]}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC9kV5SXQCu0",
        "outputId": "4ab99e9b-2778-4010-c7d5-ca27bfb96b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masukkan kalimat baru: aplikasi berguna, bagus dan bermanfaat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "Sentimen kalimat baru adalah positif.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FastText dan GRU (80:20)"
      ],
      "metadata": {
        "id": "SHzH-ikAQ76s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import FastText\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Tokenisasi teks\n",
        "df_reviews['tokens'] = df_reviews['text_akhir'].apply(nltk.word_tokenize)\n",
        "\n",
        "# Buat dan latih model FastText untuk mendapatkan representasi vektor kata\n",
        "model_ft = FastText(sentences=df_reviews['tokens'].tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
        "model_ft.save(\"fasttext.model\")  # Simpan model untuk penggunaan selanjutnya\n",
        "\n",
        "# Buat kamus kata dan ubah teks menjadi urutan angka\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_reviews['text_akhir'].tolist())\n",
        "sequences = tokenizer.texts_to_sequences(df_reviews['text_akhir'].tolist())\n",
        "\n",
        "# Padding urutan agar memiliki panjang yang sama\n",
        "max_length = max([len(seq) for seq in sequences]) # Tentukan panjang maksimum urutan\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Buat matriks embedding\n",
        "vocab_size = len(tokenizer.word_index) + 1 # Ukuran kamus berdasarkan jumlah kata unik\n",
        "embedding_matrix = np.zeros((vocab_size, 100)) # Membuat matriks embedding dengan ukuran (vocab_size, 100) untuk vektor 100 dimensi\n",
        "\n",
        "# Mengisi matriks embedding dengan vektor kata dari model FastText\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in model_ft.wv:\n",
        "        embedding_matrix[i] = model_ft.wv[word]\n",
        "\n",
        "# Menyiapkan data untuk pelatihan ( X adalah urutan kata yang telah diproses, dan y adalah target variabel (polaritas sentimen))\n",
        "X = padded_sequences\n",
        "y = df_reviews['polarity']\n",
        "\n",
        "# 8. Bagi data menjadi data latih dan data uji\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Menampilkan kata-kata (features) yang ada dalam tokenizer\n",
        "print(\"Kata-kata yang digunakan dalam Tokenizer:\")\n",
        "print(list(tokenizer.word_index.keys())[:20])  # Menampilkan 20 kata pertama\n",
        "\n",
        "# Menampilkan vektor kata (embedding) untuk beberapa kata tertentu\n",
        "words_to_check = ['tolong', 'admin', 'pelit', 'buruk']  # Misalnya, beberapa kata yang ingin kita periksa\n",
        "print(\"\\nRepresentasi vektor kata untuk beberapa kata:\")\n",
        "for word in words_to_check:\n",
        "    if word in model_ft.wv:\n",
        "        print(f\"{word}: {model_ft.wv[word]}\")  # Menampilkan vektor kata untuk setiap kata\n",
        "\n",
        "# Menampilkan beberapa baris pertama dari matriks embedding\n",
        "print(\"\\nBeberapa baris pertama dari matriks embedding:\")\n",
        "df_embedding = pd.DataFrame(embedding_matrix[:10], columns=[f\"Dim_{i+1}\" for i in range(100)])  # Menampilkan 10 baris pertama\n",
        "print(df_embedding.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I0MdcMrQYbU",
        "outputId": "c6c73b13-1a48-4141-8412-373e2646efaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kata-kata yang digunakan dalam Tokenizer:\n",
            "['bagus', 'capcut', 'aplikasi', 'banget', 'iklan', 'apk', 'tolong', 'profesional', 'ngedit', 'suka', 'video', 'pakai', 'download', 'kasih', 'pas', 'template', 'edit', 'gk', 'update', 'fitur']\n",
            "\n",
            "Representasi vektor kata untuk beberapa kata:\n",
            "tolong: [-1.3105681  -2.1097655   1.6619929  -0.19410652 -0.12008878 -1.2384886\n",
            " -0.38875756 -1.9667702   0.7712515   1.5541059  -2.0329566  -1.5838144\n",
            " -0.5149621   1.4810839   1.2074851   1.8831038  -1.8796097  -0.02997081\n",
            "  0.48159003  1.7175249   1.6678241  -0.20527315 -0.28696814 -0.01269771\n",
            " -1.2249693   3.1279821   0.22121093  1.3939767   1.9057071   1.1801018\n",
            " -1.1688951   0.50229377  0.3475333  -0.4894828  -0.79325604  0.15736254\n",
            "  1.1099997  -0.41944954 -0.4919234  -0.3631457   1.86371    -1.2326365\n",
            " -0.976359    1.2882895   3.22137     0.18394323 -0.5256958   0.24123687\n",
            " -0.8033752  -3.4286387  -1.0179617   1.7629826   0.05549209  2.1676986\n",
            "  0.48643532  0.02741968 -0.22135378  1.0261544   1.120246   -0.7091473\n",
            " -0.1305418   0.4263233   0.7282133   0.42520794 -0.31841144  1.106528\n",
            " -0.00710999  1.9260366   2.1590085   2.6445696  -0.65007573  1.6089675\n",
            "  0.84384495 -0.49157104 -0.2901363   1.1395532   1.7139347   0.9348089\n",
            " -0.31204405 -1.9529718  -1.4462562   0.02557438 -0.587777   -0.66614705\n",
            " -1.7006216  -0.04457914  1.3125314   0.89501643 -0.16789243  0.78339946\n",
            " -0.42743868 -0.33620042  1.6466414  -2.2708898   0.48708314  3.2518167\n",
            " -0.6253982   0.33311442  0.2530307  -0.16269538]\n",
            "admin: [ 0.54225284 -1.3746076   1.6110919  -0.20968148 -0.44700015  1.1345607\n",
            "  0.9843794  -1.723917   -0.11535918  1.042917   -0.83766186 -0.74342155\n",
            "  0.6093766  -0.27988607  0.18767264  2.0248249   0.07677906 -0.8305856\n",
            " -0.296179    1.6831033   1.0603833  -0.6762858  -0.5088303   0.0417945\n",
            " -0.64703524  1.1289608   0.52109176  0.5562125  -0.08608188  0.41292298\n",
            " -0.3028938   0.72530276  1.8380965   0.41766977 -1.6733415   0.23763055\n",
            "  0.1877532   0.3077941  -0.22921045 -0.5171073   1.6929519   0.5083256\n",
            " -0.9093358  -0.05092443  1.9141617   0.85815895  0.10727644 -0.9016754\n",
            " -0.64623195 -1.5046504  -1.5904567   0.3201889   0.44120744  1.5081043\n",
            " -0.04838586 -0.09448123  0.12564638  1.1834533   0.41523778  0.67437774\n",
            " -0.39955252 -0.21122417  0.01107713 -0.2635158  -0.2738387  -0.38567474\n",
            "  0.29447788  0.5822896   1.1508975   0.53930247  0.76081073 -0.12928239\n",
            " -0.37102494 -0.30121067  0.09855064  0.3646912   0.05177259 -0.10262763\n",
            " -0.43649    -0.7600559  -0.00810672 -1.1232646  -0.40529433 -0.1725947\n",
            " -0.61104494  0.2718548   0.5562447  -1.0573021  -0.03480712  0.66512513\n",
            " -1.2552826  -0.01027685  0.58599174 -0.11516882 -1.0000584   0.5112874\n",
            " -0.45541704  0.5674871  -1.3859456   0.70601344]\n",
            "pelit: [ 0.51084167 -1.3817374   1.401294   -0.21490481 -0.17980467  0.6163834\n",
            "  0.67631465 -0.6270893  -0.08396224  0.6677372  -0.1634317   0.49884957\n",
            "  2.1483455  -1.6732457  -0.30875787  1.299333    1.5995048  -0.37522635\n",
            " -0.44767532  0.47845355 -0.78188664  1.030685   -0.57312506  1.1918632\n",
            " -1.1181754   1.4313174   0.6895482  -0.20572431 -1.349522   -0.15444991\n",
            " -0.3178185  -0.4559702   1.2409414   0.49267578 -0.7224181   0.21945913\n",
            "  0.54167485 -0.642362   -0.09970219 -0.09168208  0.8152263   0.6049085\n",
            " -0.43097782 -0.973026    1.2197407   1.916595    0.6901842   1.0643028\n",
            " -0.16776538  0.14889543 -1.252153    0.04512153 -1.3851211   2.1750612\n",
            "  0.23569135  1.5053054  -0.25459847  0.9744726  -0.27686054 -0.3090977\n",
            " -0.20625758 -0.11652568  0.49770465  0.01313229  0.11513419 -0.07279722\n",
            "  1.0831338   0.7458875  -0.1204678   0.99869347  0.17927478  0.6256972\n",
            "  0.26208794  0.02833728  1.5853283   0.5106174   1.6196896  -0.17735763\n",
            " -1.2546809   0.22514793  0.0045606  -0.68353295 -0.6690047   0.6015605\n",
            " -0.11670669 -0.46487388  0.16325763 -0.6673959  -0.02377707 -0.09375198\n",
            " -0.05070329  0.67661434 -0.39229405 -0.92437947 -1.3867364  -0.7951897\n",
            " -1.3155496   0.10386334 -1.1679235   0.08034311]\n",
            "buruk: [-0.43601462 -0.21533984  0.36875826  0.15820858  1.0960515  -0.15698801\n",
            " -0.6540613   0.18327895  1.5055046  -0.9101505  -0.96477914 -0.00782971\n",
            "  0.06825005 -0.62735444 -1.1789634   0.2842849   0.34146595 -0.35887495\n",
            " -0.09424317 -0.12503442 -0.0803455   0.28429604  0.27494764  0.21410237\n",
            " -0.11310773 -0.23880777 -0.37819684  1.2813935  -0.43134165  1.8238492\n",
            " -0.7475606  -0.18822059  0.663249   -1.2246186   0.8820584  -0.35074127\n",
            "  1.1426377  -0.9084222  -0.07525086 -0.583786   -0.3219844   0.24323957\n",
            " -1.1551058  -0.2750511  -0.20829214  0.4339467   0.37404498 -0.8067552\n",
            " -0.21081544 -0.50807685 -1.0817116   0.49339232 -1.2694378   0.9201599\n",
            " -0.86525726  0.06038904 -0.37371287 -1.3063015   0.9877547  -0.21689479\n",
            "  1.1585256   1.1905277  -0.50277406  1.1758195   0.01876428  0.3152518\n",
            " -0.50753504  0.9127019  -0.39015812  1.4955986  -0.7197229   2.2546484\n",
            "  1.1717372  -0.5996775   0.7362535  -0.00520552  1.0727088   0.5233662\n",
            " -0.53122497 -0.5361287   1.1849493  -0.54554385 -0.7988973  -0.09250652\n",
            "  1.211216   -0.34776416  0.15395226  0.43069082 -0.39732724 -0.8418195\n",
            " -0.02040625  1.2026044  -0.81859225 -0.8759811  -0.13689111  0.31897318\n",
            " -0.10808924 -0.5833425  -0.7685577   0.09172331]\n",
            "\n",
            "Beberapa baris pertama dari matriks embedding:\n",
            "      Dim_1     Dim_2     Dim_3     Dim_4     Dim_5     Dim_6     Dim_7  \\\n",
            "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "1  0.700149  0.236198  0.602068 -0.985269  1.721145  1.429587 -1.085546   \n",
            "2  1.782136 -0.686873  1.626928  1.070526  0.028237  0.907035  0.531450   \n",
            "3  0.932662  0.664164  0.799449  1.422455  0.254798  0.159687  0.141799   \n",
            "4 -1.102668  0.217392 -0.090435 -0.662440  0.184747  2.055951  0.596349   \n",
            "\n",
            "      Dim_8     Dim_9    Dim_10  ...    Dim_91    Dim_92    Dim_93    Dim_94  \\\n",
            "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "1 -0.478128 -0.240845 -0.137132  ... -0.831241  0.276371 -1.306454 -1.078230   \n",
            "2 -1.493460 -0.117450  0.544353  ...  0.909092 -0.506123  0.121865 -1.374801   \n",
            "3 -0.653241 -0.568914 -0.043307  ...  0.624532  0.225614 -0.652378 -2.065260   \n",
            "4 -0.888371 -0.359708  1.190706  ... -1.642388 -0.398813 -1.644215 -0.198990   \n",
            "\n",
            "     Dim_95    Dim_96    Dim_97    Dim_98    Dim_99   Dim_100  \n",
            "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "1  0.140385  0.527661 -0.312109  0.303983  0.121633 -1.226687  \n",
            "2 -0.168541 -0.805918 -0.054690  1.764896 -0.565488  0.029824  \n",
            "3 -0.045768  0.650385  0.250522  1.230969 -1.228037 -1.412351  \n",
            "4 -0.313595 -0.842632 -0.682658  0.074379 -2.839831  0.734885  \n",
            "\n",
            "[5 rows x 100 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Bidirectional, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Definisikan Custom Callback untuk menghentikan pelatihan ketika akurasi mencapai threshold tertentu\n",
        "class MyThresholdCallback(Callback):\n",
        "    def __init__(self, threshold):\n",
        "        super(MyThresholdCallback, self).__init__()\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        accuracy = logs[\"accuracy\"]\n",
        "        val_accuracy = logs[\"val_accuracy\"]\n",
        "        # Jika akurasi latih dan validasi mencapai threshold, hentikan pelatihan\n",
        "        if accuracy >= self.threshold and val_accuracy >= self.threshold:\n",
        "            self.model.stop_training = True\n",
        "            print(f\"\\nPelatihan dihentikan pada epoch {epoch + 1} karena akurasi latihan dan validasi mencapai {self.threshold * 100:.2f}%\")\n",
        "\n",
        "# Define the embedding dimension\n",
        "embedding_dim = 100  # Menyesuaikan dengan dimensi FastText\n",
        "\n",
        "# Membuat model GRU dengan Bidirectional\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=X_train.shape[1], weights=[embedding_matrix], trainable=False))\n",
        "model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
        "model.add(Dropout(0.3))   # Dropout layer untuk menghindari overfitting\n",
        "model.add(BatchNormalization())  # Normalisasi lapisan untuk stabilitas pelatihan\n",
        "model.add(Bidirectional(GRU(64)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(3, activation='softmax'))  # Layer output dengan 3 kelas: positif, negatif, netral\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Inisialisasi Custom Callback dengan threshold akurasi 0.88\n",
        "my_callback = MyThresholdCallback(threshold=0.86)\n",
        "\n",
        "# Inisialisasi LabelEncoder untuk mengubah label menjadi representasi numerik\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "# Melatih model\n",
        "history = model.fit(X_train, y_train_encoded, epochs=20, batch_size=64, validation_split=0.2, callbacks=[my_callback])\n",
        "\n",
        "# Menampilkan akurasi\n",
        "train_acc = history.history['accuracy'][-1]\n",
        "val_acc = history.history['val_accuracy'][-1]\n",
        "\n",
        "print(f'Akurasi Training: {train_acc*100:.2f}%')\n",
        "print(f'Akurasi Validasi: {val_acc*100:.2f}%')\n",
        "\n",
        "# Evaluasi model\n",
        "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL0oT6BHDGof",
        "outputId": "ec911178-5266-4e13-a3b8-670b1cba3e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 533ms/step - accuracy: 0.7049 - loss: 0.7186 - val_accuracy: 0.7776 - val_loss: 0.5809\n",
            "Epoch 2/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m480s\u001b[0m 548ms/step - accuracy: 0.8139 - loss: 0.4931 - val_accuracy: 0.8318 - val_loss: 0.4499\n",
            "Epoch 3/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m504s\u001b[0m 551ms/step - accuracy: 0.8360 - loss: 0.4411 - val_accuracy: 0.8443 - val_loss: 0.4160\n",
            "Epoch 4/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m482s\u001b[0m 551ms/step - accuracy: 0.8541 - loss: 0.4000 - val_accuracy: 0.8526 - val_loss: 0.3971\n",
            "Epoch 5/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 552ms/step - accuracy: 0.8582 - loss: 0.3841 - val_accuracy: 0.8471 - val_loss: 0.4134\n",
            "Epoch 6/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 551ms/step - accuracy: 0.8670 - loss: 0.3600 - val_accuracy: 0.8455 - val_loss: 0.4116\n",
            "Epoch 7/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 537ms/step - accuracy: 0.8749 - loss: 0.3412 - val_accuracy: 0.8514 - val_loss: 0.3934\n",
            "Epoch 8/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 502ms/step - accuracy: 0.8827 - loss: 0.3181\n",
            "Pelatihan dihentikan pada epoch 8 karena akurasi latihan dan validasi mencapai 86.00%\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 537ms/step - accuracy: 0.8827 - loss: 0.3181 - val_accuracy: 0.8628 - val_loss: 0.3958\n",
            "Akurasi Training: 88.20%\n",
            "Akurasi Validasi: 86.28%\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 67ms/step - accuracy: 0.8651 - loss: 0.3954\n",
            "Test Loss: 0.3973\n",
            "Test Accuracy: 86.39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk mode GRU saya menargetkan hanya sampai 86% karena akurasi paling tinggi apabila dijankan seluruh epochnya hanya sampai 88% dan membutuhkan waktu yang lama"
      ],
      "metadata": {
        "id": "nkhoepjIQ8VE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk preprocessing kalimat baru\n",
        "\n",
        "def preprocess_kalimat(kalimat_baru):\n",
        "    # Melakukan preprocessing pada kalimat baru\n",
        "    kalimat_baru_clean_and_casefold = clean_and_casefold(kalimat_baru)\n",
        "    kalimat_baru_fix_slangwords = fix_slangwords(kalimat_baru_clean_and_casefold)\n",
        "    kalimat_baru_tokenize_and_filter = tokenize_and_filter(kalimat_baru_fix_slangwords)\n",
        "    kalimat_baru_final = to_sentence(kalimat_baru_tokenize_and_filter)\n",
        "    return kalimat_baru_final\n",
        "\n",
        "# Input kalimat baru dari pengguna\n",
        "kalimat_baru = input(\"Masukkan kalimat baru: \")\n",
        "\n",
        "# Preprocessing kalimat baru\n",
        "kalimat_baru_processed = preprocess_kalimat(kalimat_baru)\n",
        "\n",
        "# Tokenisasi kalimat baru dengan tokenizer yang sudah dilatih sebelumnya\n",
        "sequences_baru = tokenizer.texts_to_sequences([kalimat_baru_processed])\n",
        "\n",
        "# Padding urutan kalimat baru agar memiliki panjang yang sama\n",
        "X_kalimat_baru = pad_sequences(sequences_baru, maxlen=X_train.shape[1], padding='post')\n",
        "\n",
        "# Menggunakan model untuk memprediksi sentimen\n",
        "prediksi_sentimen = model.predict(X_kalimat_baru)\n",
        "\n",
        "# Mengonversi prediksi (output) menjadi label\n",
        "predicted_class = np.argmax(prediksi_sentimen, axis=1)\n",
        "\n",
        "# Mendapatkan label asli (positif, negatif, netral)\n",
        "sentimen_labels = ['negatif', 'netral', 'positif']\n",
        "\n",
        "# Menampilkan hasil prediksi\n",
        "print(f\"Sentimen kalimat baru adalah {sentimen_labels[predicted_class[0]]}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff093401-f0ea-409f-c37e-e94779d648e1",
        "id": "7aByhWj8FFBe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masukkan kalimat baru: aplikasi kikir, pelit dan buruk\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 931ms/step\n",
            "Sentimen kalimat baru adalah negatif.\n"
          ]
        }
      ]
    }
  ]
}